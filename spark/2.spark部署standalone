1、Stanalone模式安装
1、启动你的虚拟机
2、下载spark的安装包（可以直接从资料里面拷贝）
3、将安装包解压到安装目录
4、将conf目录下的slaves.template 复制为slaves,将slave节点的主机名写入
5、将cong目录下的spark.env.sh.template 复制为spark.evn.sh ，SPARK.MASTER.HOST SPARK.MASTER.PORT 写入
6、将整个目录分发到slave节点上，
7、在master节点上执行sbin/start-all.sh 启动整个集群
8、通过http://master:8080来访问spark的http界面。

java的安装：
1、sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/java/bin/java 1070 
      sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java/bin/javac 1070
    sudo update-alternatives --config java
2、如何配置日志服务器？
      1、复制spark.default.conf.template 到 spark.default.conf
      2、将以下添加到spark.default.conf文件中
	  spark.eventLog.enabled  true    开启日志
	 spark.eventLog.dir       hdfs://master01:9000/directory   日志存在哪里？【目录需要有】
 
3、修改spark.env.sh
     export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=4000
-Dspark.history.retainedApplications=3
-Dspark.history.fs.logDirectory=hdfs://master01:9000/directory"
   4、将所有修改的文件同步到其他节点

   5、在master上start-all.sh 启动集群，。  在master上  start-history-server.sh  启动日志服务器，在4000端口可查。


3、Standalone HA配置
1、修改spark.env.sh 配置文件，注释掉 SPARK.MASTER,HOST
2、添加
export SPARK_DAEMON_JAVA_OPTS="
 -Dspark.deploy.recoveryMode=ZOOKEEPER
 -Dspark.deploy.zookeeper.url=zk1,zk2,zk3
 -Dspark.deploy.zookeeper.dir=/spark"

     3、分发你的配置到所有节点
     4、启动  1、start-all.sh 启动所有节点，   手动选择一个其他节点执行  start-master.sh
     5、启用HA之后，master的访问变成  spark://master01:7077，master02:7077


4、yarn集群的配置
     1、修改yarn的配置，启动yarn
     2、修改提交应用的client下  conf目录下的spark.env.sh  添加：
HADOOP_CONF_DIR=/home/bigdata/hadoop/hadoop-2.7.3/etc/hadoop
YARN_CONF_DIR=/home/bigdata/hadoop/hadoop-2.7.3/etc/hadoop
